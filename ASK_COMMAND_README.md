# Commande ASK - Documentation

La commande `llm ask` permet de poser une question unique √† un mod√®le LLM et d'obtenir une r√©ponse imm√©diate avec streaming en temps r√©el.

## Installation

Assurez-vous d'avoir install√© les d√©pendances :

```bash
pip install -r requirements.txt
```

## Configuration

D√©finissez votre cl√© API OpenRouter :

```bash
export OPENROUTER_API_KEY="your-openrouter-api-key-here"
```

## Usage de base

### Syntaxe

```bash
python -m llm.cli.ask "<question>" --model <MODEL_ID> [--system "<system_prompt>"] [--json]
```

### Param√®tres

- `question` : Votre question (obligatoire)
- `--model` / `-m` : Identifiant du mod√®le √† utiliser (obligatoire)
- `--system` / `-s` : Prompt syst√®me optionnel
- `--json` : Sortie en format JSON pour l'int√©gration dans des scripts

## Exemples d'utilisation

### 1. Question simple

```bash
python -m llm.cli.ask "What is 2+2?" --model openai/gpt-4o-mini
```

**Sortie :**
```
Question:
> What is 2+2?

Response:

2+2 equals 4.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Model: openai/gpt-4o-mini | Tokens: 12‚Üí6 (18) | 1.23s | ~$0.0001
```

### 2. Avec prompt syst√®me

```bash
python -m llm.cli.ask "Explain quantum physics" --model openai/gpt-4o-mini --system "You are a physics teacher for high school students"
```

### 3. Sortie JSON

```bash
python -m llm.cli.ask "What is AI?" --model openai/gpt-4o-mini --json
```

**Sortie :**
```json
{
  "question": "What is AI?",
  "response": "Artificial Intelligence (AI) refers to...",
  "model": "openai/gpt-4o-mini",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 120,
    "total_tokens": 135
  },
  "latency_ms": 1234.5,
  "estimated_cost_usd": 0.0045,
  "system": null
}
```

### 4. Question longue avec √©chappement

```bash
python -m llm.cli.ask "Can you write a Python function to calculate fibonacci numbers and explain how it works?" --model anthropic/claude-3-haiku
```

## Mod√®les support√©s

La commande supporte tous les mod√®les disponibles via OpenRouter. Exemples populaires :

- `openai/gpt-4o-mini` - Mod√®le rapide et √©conomique
- `openai/gpt-4o` - Mod√®le le plus capable d'OpenAI
- `anthropic/claude-3-haiku` - Mod√®le rapide d'Anthropic
- `anthropic/claude-3.5-sonnet` - Mod√®le √©quilibr√© d'Anthropic
- `meta-llama/llama-3.1-70b-instruct` - Mod√®le open source

## Fonctionnalit√©s

### Mode Streaming

Par d√©faut, la r√©ponse s'affiche en temps r√©el au fur et √† mesure de sa g√©n√©ration, offrant une exp√©rience interactive.

### M√©tadonn√©es compl√®tes

Apr√®s chaque r√©ponse, vous obtenez :
- Mod√®le utilis√©
- Nombre de tokens (prompt ‚Üí completion)
- Temps de r√©ponse
- Co√ªt estim√©

### Gestion d'erreurs

La commande g√®re intelligemment les erreurs avec des messages d'aide :

```bash
# Mod√®le invalide
Error: Model 'invalid-model' not found
üí° Tip: Use `llm models list` to see available models

# Probl√®me d'authentification
Error: Authentication error (401): Check your OpenRouter API key
üí° Tip: Check your OPENROUTER_API_KEY environment variable

# Rate limiting
Error: Rate limit exceeded (429): Too many requests
üí° Tip: Wait a moment and try again
```

## Int√©gration dans des scripts

### Bash

```bash
#!/bin/bash

# Obtenir une r√©ponse JSON
RESPONSE=$(python -m llm.cli.ask "What's the current date format?" --model openai/gpt-4o-mini --json)

# Parser avec jq
ANSWER=$(echo "$RESPONSE" | jq -r '.response')
COST=$(echo "$RESPONSE" | jq -r '.estimated_cost_usd')

echo "Answer: $ANSWER"
echo "Cost: \$$COST"
```

### Python

```python
import subprocess
import json

def ask_llm(question, model="openai/gpt-4o-mini", system=None):
    cmd = [
        "python", "-m", "llm.cli.ask",
        question,
        "--model", model,
        "--json"
    ]
    
    if system:
        cmd.extend(["--system", system])
    
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    return json.loads(result.stdout)

# Usage
try:
    response = ask_llm("Explain machine learning in one paragraph")
    print(f"Response: {response['response']}")
    print(f"Tokens used: {response['usage']['total_tokens']}")
    print(f"Cost: ${response['estimated_cost_usd']:.4f}")
except subprocess.CalledProcessError as e:
    print(f"Error: {e}")
```

### Node.js

```javascript
const { exec } = require('child_process');
const util = require('util');
const execAsync = util.promisify(exec);

async function askLLM(question, model = 'openai/gpt-4o-mini') {
    const cmd = `python -m llm.cli.ask "${question}" --model ${model} --json`;
    
    try {
        const { stdout } = await execAsync(cmd);
        return JSON.parse(stdout);
    } catch (error) {
        throw new Error(`LLM Error: ${error.message}`);
    }
}

// Usage
askLLM("What is Node.js?")
    .then(response => {
        console.log('Response:', response.response);
        console.log('Cost: $' + response.estimated_cost_usd.toFixed(4));
    })
    .catch(console.error);
```

## Cas d'usage avanc√©s

### 1. Processing pipeline

```bash
# Traitement en cha√Æne
python -m llm.cli.ask "Summarize: $(cat article.txt)" --model anthropic/claude-3-haiku --json | jq -r '.response' > summary.txt
```

### 2. Interactive scripting

```bash
#!/bin/bash
echo "AI Assistant - Ask anything!"
while true; do
    read -p "Question: " question
    if [ "$question" = "exit" ]; then
        break
    fi
    python -m llm.cli.ask "$question" --model openai/gpt-4o-mini
    echo
done
```

### 3. Batch processing

```python
import json
from concurrent.futures import ThreadPoolExecutor

questions = [
    "What is Python?",
    "What is JavaScript?",
    "What is Go?"
]

def process_question(question):
    # Use the ask command programmatically
    result = ask_llm(question)
    return {
        'question': question,
        'answer': result['response'],
        'cost': result['estimated_cost_usd']
    }

# Process in parallel
with ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(process_question, questions))

total_cost = sum(r['cost'] for r in results)
print(f"Processed {len(results)} questions for ${total_cost:.4f}")
```

## Performance et co√ªts

### Optimisation des co√ªts

1. **Choisir le bon mod√®le** : Utilisez `openai/gpt-4o-mini` pour des t√¢ches simples
2. **Questions pr√©cises** : R√©duisez les tokens inutiles
3. **Prompt syst√®me efficace** : Un bon prompt syst√®me peut r√©duire les tokens de r√©ponse

### Mod√®les recommand√©s par usage

| Usage | Mod√®le recommand√© | Co√ªt relatif |
|-------|------------------|--------------|
| Questions simples | `openai/gpt-4o-mini` | Tr√®s bas |
| Analyse de code | `anthropic/claude-3.5-sonnet` | Moyen |
| T√¢ches complexes | `openai/gpt-4o` | √âlev√© |
| Traduction | `openai/gpt-4o-mini` | Tr√®s bas |
| Cr√©ativit√© | `anthropic/claude-3.5-sonnet` | Moyen |

## D√©pannage

### Probl√®mes courants

1. **"Module not found"**
   ```bash
   # S'assurer d'√™tre dans le bon r√©pertoire
   cd /path/to/phemis
   python -m llm.cli.ask "test" --model openai/gpt-4o-mini
   ```

2. **"Authentication error"**
   ```bash
   # V√©rifier la cl√© API
   echo $OPENROUTER_API_KEY
   export OPENROUTER_API_KEY="your-key-here"
   ```

3. **"Rate limit exceeded"**
   - Attendre quelques secondes entre les requ√™tes
   - Utiliser un mod√®le moins demand√©

4. **"Model not found"**
   - V√©rifier l'identifiant exact du mod√®le
   - Certains mod√®les peuvent √™tre temporairement indisponibles

### Debug mode

Pour plus d'informations de d√©bogage, vous pouvez examiner les r√©ponses en mode verbose :

```python
# Dans example_ask_usage.py
export PYTHONPATH=.
python example_ask_usage.py
```

## Contributing

Pour √©tendre la commande ask :

1. Modifier `/Users/emi/Desktop/phemis/llm/cli/ask.py`
2. Ajouter des tests dans `test_ask.py`
3. Mettre √† jour cette documentation

## Licence

Ce projet fait partie du framework LLM CLI sous licence MIT.